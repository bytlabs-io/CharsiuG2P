{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recreational-easter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import argparse\n",
    "from jiwer import wer,cer\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "from datasets import load_metric\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Union, Dict, List, Optional\n",
    "from transformers import AdamW, AutoTokenizer, T5ForConditionalGeneration, T5Config\n",
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    DataCollator,\n",
    "    Seq2SeqTrainer, \n",
    "    Seq2SeqTrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "import sys\n",
    "sys.path.append('src')\n",
    "from data_utils import load_pronuncation_dictionary\n",
    "\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    \n",
    "    batch['input_ids'] = batch['word']\n",
    "    batch['labels'] = batch['pron']\n",
    "    \n",
    "    return batch\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class DataCollatorWithPadding:\n",
    "\n",
    "    tokenizer: AutoTokenizer\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        words = [feature[\"input_ids\"] for feature in features]\n",
    "        prons = [feature[\"labels\"] for feature in features]\n",
    "\n",
    "        batch = self.tokenizer(words,padding=self.padding,add_special_tokens=False,\n",
    "                          return_attention_mask=True,return_tensors='pt')\n",
    "        pron_batch = self.tokenizer(prons,padding=self.padding,add_special_tokens=True,\n",
    "                          return_attention_mask=True,return_tensors='pt')\n",
    "        \n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        batch['labels'] = pron_batch['input_ids'].masked_fill(pron_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "\n",
    "        return batch\n",
    "    \n",
    "    \n",
    "\n",
    "                \n",
    "                \n",
    "def evaluate_all_metrics(model,dataset,args):\n",
    "    \n",
    "    model.eval()\n",
    "    words = tokenizer('힐끔힐끔', return_tensors=\"pt\")\n",
    "    out = model.generate(**words.to(device),num_beams=5).squeeze()\n",
    "    tokenizer.decode(out,skip_special_tokens=True)\n",
    "    return \n",
    "        \n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"cer\": cer}\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    '''\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('integers', metavar='N', type=int, nargs='+',\n",
    "                        help='an integer for the accumulator')\n",
    "    parser.add_argument('grad_acc', type=int, default=8,\n",
    "                        help='an integer for the accumulator')    \n",
    "    args = parser.parse_args()\n",
    "    '''\n",
    "    \n",
    "    cer_metric = load_metric(\"cer\")\n",
    "    '''\n",
    "    hypotheses = ['ki:m˥','a:m˥']\n",
    "    ground_truth = ['hi:m˥','a:m˥']\n",
    "    wer(ground_truth,hypotheses)\n",
    "    cer(ground_truth,hypotheses)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    data = load_pronuncation_dictionary('other_dicts/fr-fr')\n",
    "    data = data.map(prepare_dataset)    \n",
    "    data = data.shuffle(seed=666)\n",
    "    train_dataset = data.select([i for i in range(200000)])\n",
    "    eval_dataset = data.select([i for i in range(200000,len(data))])\n",
    "    \n",
    "    config = T5Config.from_pretrained('google/byt5-small')\n",
    "    #model = T5ForConditionalGeneration.from_pretrained('google/byt5-small')\n",
    "    \n",
    "    \n",
    "    config\n",
    "    config.num_decoder_layers = 2\n",
    "    config.num_layers = 6\n",
    "    config.d_kv = 32\n",
    "    config.d_model = 256\n",
    "    config.d_ff = 256\n",
    "\n",
    "    model = T5ForConditionalGeneration(config)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained('google/byt5-small')\n",
    "\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    \n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        predict_with_generate=True,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        per_device_train_batch_size=256,\n",
    "        per_device_eval_batch_size=512,\n",
    "        num_train_epochs=100,\n",
    "        gradient_accumulation_steps=2,\n",
    "        learning_rate=3e-4,\n",
    "        weight_decay=1e-6,\n",
    "        warmup_steps=1000,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        fp16=True, \n",
    "        output_dir=\"./results\",\n",
    "        logging_steps=500,\n",
    "        save_steps=2000,\n",
    "        eval_steps=2000,\n",
    "        save_total_limit=2,\n",
    "        metric_for_best_model='cer'\n",
    "    )\n",
    "    \n",
    "    \n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-contrary",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset[26281]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-pierce",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "model.eval()\n",
    "words = tokenizer('craignîtes', return_tensors=\"pt\")\n",
    "out = model.generate(**words.to(device),num_beams=5,num_return_sequences=3).squeeze()\n",
    "[tokenizer.decode(i,skip_special_tokens=True) for i in out]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
