{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db828d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/train.py --output_dir /scratch/lingjzhu_root/lingjzhu1/lingjzhu/g2p/byt5_12_layers_baseline --num_encoder_layers 8 --num_decoder_layers 4 --d_ff 1024  --model byt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da120588",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/train.py --output_dir /scratch/lingjzhu_root/lingjzhu1/lingjzhu/g2p/byt5_8_layers_baseline  --model byt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01396b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/train.py --output_dir /scratch/lingjzhu_root/lingjzhu1/lingjzhu/g2p/byt5_16_layers_baseline --train --num_encoder_layers 12 --num_decoder_layers 4 --d_ff 1024 --model byt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c596a7b-954a-4c0d-992f-d619cb5aa4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 99/99 [00:47<00:00,  2.10it/s]\n",
      "100%|██████████████████████████████| 7165398/7165398 [07:11<00:00, 16604.95ex/s]\n",
      "100%|██████████████████████████████████████████| 99/99 [00:00<00:00, 413.87it/s]\n",
      "100%|████████████████████████████████████| 4950/4950 [00:00<00:00, 16645.22ex/s]\n",
      "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n",
      "Initializing a ByT5 model...\n",
      "The following columns in the training set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: pron, language, word, variant.\n",
      "/home/lingjzhu/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 7165398\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 512\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 139950\n",
      "{'loss': 5.5981, 'learning_rate': 0.0003, 'epoch': 0.07}                        \n",
      "{'loss': 3.0697, 'learning_rate': 0.0002999616623572683, 'epoch': 0.14}         \n",
      "{'loss': 2.7399, 'learning_rate': 0.00029984666902607135, 'epoch': 0.21}        \n",
      "{'loss': 2.4646, 'learning_rate': 0.0002996550787873857, 'epoch': 0.29}         \n",
      "{'loss': 2.2298, 'learning_rate': 0.0002993869895761197, 'epoch': 0.36}         \n",
      "  4%|█▏                                | 5000/139950 [48:10<21:05:38,  1.78it/s]The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: pron, language, word, variant.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4950\n",
      "  Batch size = 128\n",
      "\n",
      "  0%|                                                    | 0/39 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|██▎                                         | 2/39 [00:01<00:23,  1.61it/s]\u001b[A\n",
      "  8%|███▍                                        | 3/39 [00:02<00:32,  1.12it/s]\u001b[A\n",
      " 10%|████▌                                       | 4/39 [00:03<00:36,  1.05s/it]\u001b[A\n",
      " 13%|█████▋                                      | 5/39 [00:05<00:38,  1.13s/it]\u001b[A\n",
      " 15%|██████▊                                     | 6/39 [00:06<00:38,  1.17s/it]\u001b[A\n",
      " 18%|███████▉                                    | 7/39 [00:07<00:38,  1.19s/it]\u001b[A\n",
      " 21%|█████████                                   | 8/39 [00:08<00:37,  1.22s/it]\u001b[A\n",
      " 23%|██████████▏                                 | 9/39 [00:10<00:36,  1.21s/it]\u001b[A\n",
      " 26%|███████████                                | 10/39 [00:11<00:36,  1.25s/it]\u001b[A\n",
      " 28%|████████████▏                              | 11/39 [00:12<00:34,  1.24s/it]\u001b[A\n",
      " 31%|█████████████▏                             | 12/39 [00:13<00:32,  1.21s/it]\u001b[A\n",
      " 33%|██████████████▎                            | 13/39 [00:15<00:31,  1.22s/it]\u001b[A\n",
      " 36%|███████████████▍                           | 14/39 [00:16<00:30,  1.23s/it]\u001b[A\n",
      " 38%|████████████████▌                          | 15/39 [00:17<00:29,  1.22s/it]\u001b[A\n",
      " 41%|█████████████████▋                         | 16/39 [00:18<00:28,  1.22s/it]\u001b[A\n",
      " 44%|██████████████████▋                        | 17/39 [00:19<00:27,  1.24s/it]\u001b[A\n",
      " 46%|███████████████████▊                       | 18/39 [00:21<00:26,  1.26s/it]\u001b[A\n",
      " 49%|████████████████████▉                      | 19/39 [00:22<00:24,  1.23s/it]\u001b[A\n",
      " 51%|██████████████████████                     | 20/39 [00:23<00:23,  1.22s/it]\u001b[A\n",
      " 54%|███████████████████████▏                   | 21/39 [00:24<00:21,  1.22s/it]\u001b[A\n",
      " 56%|████████████████████████▎                  | 22/39 [00:26<00:20,  1.23s/it]\u001b[A\n",
      " 59%|█████████████████████████▎                 | 23/39 [00:27<00:20,  1.26s/it]\u001b[A\n",
      " 62%|██████████████████████████▍                | 24/39 [00:28<00:18,  1.24s/it]\u001b[A\n",
      " 64%|███████████████████████████▌               | 25/39 [00:29<00:17,  1.23s/it]\u001b[A\n",
      " 67%|████████████████████████████▋              | 26/39 [00:31<00:15,  1.22s/it]\u001b[A\n",
      " 69%|█████████████████████████████▊             | 27/39 [00:32<00:14,  1.21s/it]\u001b[A\n",
      " 72%|██████████████████████████████▊            | 28/39 [00:33<00:13,  1.19s/it]\u001b[A\n",
      " 74%|███████████████████████████████▉           | 29/39 [00:34<00:11,  1.19s/it]\u001b[A\n",
      " 77%|█████████████████████████████████          | 30/39 [00:35<00:11,  1.23s/it]\u001b[A\n",
      " 79%|██████████████████████████████████▏        | 31/39 [00:37<00:09,  1.24s/it]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 32/39 [00:38<00:08,  1.23s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▍      | 33/39 [00:39<00:07,  1.23s/it]\u001b[A\n",
      " 87%|█████████████████████████████████████▍     | 34/39 [00:40<00:06,  1.20s/it]\u001b[A\n",
      " 90%|██████████████████████████████████████▌    | 35/39 [00:41<00:04,  1.21s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▋   | 36/39 [00:43<00:03,  1.22s/it]\u001b[A\n",
      " 95%|████████████████████████████████████████▊  | 37/39 [00:44<00:02,  1.23s/it]\u001b[A\n",
      " 97%|█████████████████████████████████████████▉ | 38/39 [00:45<00:01,  1.25s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 2.413818359375, 'eval_cer': 0.7715684988412261, 'eval_wer': 0.9890909090909091, 'eval_runtime': 48.1058, 'eval_samples_per_second': 102.898, 'eval_steps_per_second': 0.811, 'epoch': 0.36}\n",
      "  4%|█▏                                | 5000/139950 [48:58<21:05:38,  1.78it/s]\n",
      "100%|███████████████████████████████████████████| 39/39 [00:46<00:00,  1.13s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to /scratch/lingjzhu_root/lingjzhu1/lingjzhu/g2p/mt5_12_layers_baseline/checkpoint-5000\n",
      "Configuration saved in /scratch/lingjzhu_root/lingjzhu1/lingjzhu/g2p/mt5_12_layers_baseline/checkpoint-5000/config.json\n",
      "Model weights saved in /scratch/lingjzhu_root/lingjzhu1/lingjzhu/g2p/mt5_12_layers_baseline/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/lingjzhu_root/lingjzhu1/lingjzhu/g2p/mt5_12_layers_baseline/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/lingjzhu_root/lingjzhu1/lingjzhu/g2p/mt5_12_layers_baseline/checkpoint-5000/special_tokens_map.json\n",
      "Copy vocab file to /scratch/lingjzhu_root/lingjzhu1/lingjzhu/g2p/mt5_12_layers_baseline/checkpoint-5000/spiece.model\n",
      "{'loss': 2.0431, 'learning_rate': 0.000299042538431052, 'epoch': 0.43}          \n",
      "{'loss': 1.8946, 'learning_rate': 0.00029862190142478177, 'epoch': 0.5}         \n",
      "{'loss': 1.7728, 'learning_rate': 0.00029812529357372587, 'epoch': 0.57}        \n",
      "{'loss': 1.6729, 'learning_rate': 0.00029755296872820933, 'epoch': 0.64}        \n",
      "{'loss': 1.5873, 'learning_rate': 0.0002969052194427048, 'epoch': 0.71}         \n",
      "  7%|██▏                            | 10000/139950 [1:37:12<20:36:14,  1.75it/s]The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: pron, language, word, variant.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4950\n",
      "  Batch size = 128\n",
      "\n",
      "  0%|                                                    | 0/39 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|██▎                                         | 2/39 [00:01<00:21,  1.70it/s]\u001b[A\n",
      "  8%|███▍                                        | 3/39 [00:02<00:30,  1.17it/s]\u001b[A\n",
      " 10%|████▌                                       | 4/39 [00:03<00:34,  1.01it/s]\u001b[A\n",
      " 13%|█████▋                                      | 5/39 [00:04<00:36,  1.08s/it]\u001b[A\n",
      " 15%|██████▊                                     | 6/39 [00:06<00:36,  1.12s/it]\u001b[A\n",
      " 18%|███████▉                                    | 7/39 [00:07<00:36,  1.14s/it]\u001b[A\n",
      " 21%|█████████                                   | 8/39 [00:08<00:36,  1.18s/it]\u001b[A\n",
      " 23%|██████████▏                                 | 9/39 [00:09<00:34,  1.15s/it]\u001b[A\n",
      " 26%|███████████                                | 10/39 [00:10<00:33,  1.14s/it]\u001b[A\n",
      " 28%|████████████▏                              | 11/39 [00:11<00:32,  1.16s/it]\u001b[A\n",
      " 31%|█████████████▏                             | 12/39 [00:13<00:30,  1.14s/it]\u001b[A\n",
      " 33%|██████████████▎                            | 13/39 [00:14<00:29,  1.15s/it]\u001b[A\n",
      " 36%|███████████████▍                           | 14/39 [00:15<00:29,  1.17s/it]\u001b[A\n",
      " 38%|████████████████▌                          | 15/39 [00:16<00:28,  1.17s/it]\u001b[A\n",
      " 41%|█████████████████▋                         | 16/39 [00:17<00:27,  1.18s/it]\u001b[A\n",
      " 44%|██████████████████▋                        | 17/39 [00:18<00:26,  1.19s/it]\u001b[A\n",
      " 46%|███████████████████▊                       | 18/39 [00:20<00:25,  1.22s/it]\u001b[A\n",
      " 49%|████████████████████▉                      | 19/39 [00:21<00:23,  1.17s/it]\u001b[A\n",
      " 51%|██████████████████████                     | 20/39 [00:22<00:22,  1.16s/it]\u001b[A\n",
      " 54%|███████████████████████▏                   | 21/39 [00:23<00:21,  1.17s/it]\u001b[A\n",
      " 56%|████████████████████████▎                  | 22/39 [00:24<00:20,  1.19s/it]\u001b[A\n",
      " 59%|█████████████████████████▎                 | 23/39 [00:26<00:19,  1.23s/it]\u001b[A\n",
      " 62%|██████████████████████████▍                | 24/39 [00:27<00:17,  1.19s/it]\u001b[A\n",
      " 64%|███████████████████████████▌               | 25/39 [00:28<00:16,  1.18s/it]\u001b[A\n",
      " 67%|████████████████████████████▋              | 26/39 [00:29<00:15,  1.18s/it]\u001b[A\n",
      " 69%|█████████████████████████████▊             | 27/39 [00:30<00:14,  1.18s/it]\u001b[A\n",
      " 72%|██████████████████████████████▊            | 28/39 [00:31<00:12,  1.17s/it]\u001b[A\n",
      " 74%|███████████████████████████████▉           | 29/39 [00:33<00:11,  1.14s/it]\u001b[A\n",
      " 77%|█████████████████████████████████          | 30/39 [00:34<00:10,  1.18s/it]\u001b[A\n",
      " 79%|██████████████████████████████████▏        | 31/39 [00:35<00:09,  1.19s/it]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 32/39 [00:36<00:08,  1.18s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▍      | 33/39 [00:37<00:07,  1.19s/it]\u001b[A\n",
      " 87%|█████████████████████████████████████▍     | 34/39 [00:39<00:05,  1.17s/it]\u001b[A\n",
      " 90%|██████████████████████████████████████▌    | 35/39 [00:40<00:04,  1.18s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▋   | 36/39 [00:41<00:03,  1.19s/it]\u001b[A\n",
      " 95%|████████████████████████████████████████▊  | 37/39 [00:42<00:02,  1.19s/it]\u001b[A\n",
      " 97%|█████████████████████████████████████████▉ | 38/39 [00:43<00:01,  1.20s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.821380615234375, 'eval_cer': 0.5859241768332677, 'eval_wer': 0.9351515151515152, 'eval_runtime': 46.1411, 'eval_samples_per_second': 107.28, 'eval_steps_per_second': 0.845, 'epoch': 0.71}\n",
      "  7%|██▏                            | 10000/139950 [1:37:58<20:36:14,  1.75it/s]\n",
      "100%|███████████████████████████████████████████| 39/39 [00:45<00:00,  1.08s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to /scratch/lingjzhu_root/lingjzhu1/lingjzhu/g2p/mt5_12_layers_baseline/checkpoint-10000\n",
      "Configuration saved in /scratch/lingjzhu_root/lingjzhu1/lingjzhu/g2p/mt5_12_layers_baseline/checkpoint-10000/config.json\n",
      "Model weights saved in /scratch/lingjzhu_root/lingjzhu1/lingjzhu/g2p/mt5_12_layers_baseline/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/lingjzhu_root/lingjzhu1/lingjzhu/g2p/mt5_12_layers_baseline/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/lingjzhu_root/lingjzhu1/lingjzhu/g2p/mt5_12_layers_baseline/checkpoint-10000/special_tokens_map.json\n",
      "Copy vocab file to /scratch/lingjzhu_root/lingjzhu1/lingjzhu/g2p/mt5_12_layers_baseline/checkpoint-10000/spiece.model\n",
      "{'loss': 1.5119, 'learning_rate': 0.0002961823768262882, 'epoch': 0.79}         \n",
      "{'loss': 1.4458, 'learning_rate': 0.0002953848103733858, 'epoch': 0.86}         \n",
      "{'loss': 1.388, 'learning_rate': 0.00029451292777490066, 'epoch': 0.93}         \n",
      "{'loss': 1.3319, 'learning_rate': 0.0002935671747098137, 'epoch': 1.0}          \n",
      "{'loss': 1.2693, 'learning_rate': 0.00029254803461736643, 'epoch': 1.07}        \n",
      " 11%|███▎                           | 15000/139950 [2:26:06<20:02:22,  1.73it/s]The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: pron, language, word, variant.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4950\n",
      "  Batch size = 128\n",
      "\n",
      "  0%|                                                    | 0/39 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|██▎                                         | 2/39 [00:01<00:21,  1.70it/s]\u001b[A\n",
      "  8%|███▍                                        | 3/39 [00:02<00:29,  1.21it/s]\u001b[A\n",
      " 10%|████▌                                       | 4/39 [00:03<00:33,  1.03it/s]\u001b[A\n",
      " 13%|█████▋                                      | 5/39 [00:04<00:36,  1.07s/it]\u001b[A\n",
      " 15%|██████▊                                     | 6/39 [00:06<00:36,  1.12s/it]\u001b[A\n",
      " 18%|███████▉                                    | 7/39 [00:07<00:35,  1.12s/it]\u001b[A\n",
      " 21%|█████████                                   | 8/39 [00:08<00:36,  1.16s/it]\u001b[A\n",
      " 23%|██████████▏                                 | 9/39 [00:09<00:34,  1.15s/it]\u001b[A\n",
      " 26%|███████████                                | 10/39 [00:10<00:32,  1.13s/it]\u001b[A\n",
      " 28%|████████████▏                              | 11/39 [00:11<00:32,  1.15s/it]\u001b[A\n",
      " 31%|█████████████▏                             | 12/39 [00:12<00:30,  1.13s/it]\u001b[A\n",
      " 33%|██████████████▎                            | 13/39 [00:14<00:29,  1.13s/it]\u001b[A\n",
      " 36%|███████████████▍                           | 14/39 [00:15<00:28,  1.16s/it]\u001b[A\n",
      " 38%|████████████████▌                          | 15/39 [00:16<00:27,  1.16s/it]\u001b[A\n",
      " 41%|█████████████████▋                         | 16/39 [00:17<00:26,  1.16s/it]\u001b[A\n",
      " 44%|██████████████████▋                        | 17/39 [00:18<00:25,  1.17s/it]\u001b[A\n",
      " 46%|███████████████████▊                       | 18/39 [00:20<00:25,  1.20s/it]\u001b[A\n",
      " 49%|████████████████████▉                      | 19/39 [00:21<00:23,  1.17s/it]\u001b[A\n",
      " 51%|██████████████████████                     | 20/39 [00:22<00:21,  1.15s/it]\u001b[A\n",
      " 54%|███████████████████████▏                   | 21/39 [00:23<00:20,  1.15s/it]\u001b[A\n",
      " 56%|████████████████████████▎                  | 22/39 [00:24<00:19,  1.17s/it]\u001b[A\n",
      " 59%|█████████████████████████▎                 | 23/39 [00:25<00:19,  1.21s/it]\u001b[A\n",
      " 62%|██████████████████████████▍                | 24/39 [00:26<00:17,  1.17s/it]\u001b[A\n",
      " 64%|███████████████████████████▌               | 25/39 [00:28<00:16,  1.16s/it]\u001b[A\n",
      " 67%|████████████████████████████▋              | 26/39 [00:29<00:15,  1.16s/it]\u001b[A\n",
      " 69%|█████████████████████████████▊             | 27/39 [00:30<00:13,  1.16s/it]\u001b[A\n",
      " 72%|██████████████████████████████▊            | 28/39 [00:31<00:12,  1.12s/it]\u001b[A\n",
      " 74%|███████████████████████████████▉           | 29/39 [00:32<00:11,  1.10s/it]\u001b[A\n",
      " 77%|█████████████████████████████████          | 30/39 [00:33<00:10,  1.14s/it]\u001b[A\n",
      " 79%|██████████████████████████████████▏        | 31/39 [00:34<00:09,  1.16s/it]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 32/39 [00:36<00:08,  1.16s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▍      | 33/39 [00:37<00:06,  1.17s/it]\u001b[A\n",
      " 87%|█████████████████████████████████████▍     | 34/39 [00:38<00:05,  1.15s/it]\u001b[A\n",
      " 90%|██████████████████████████████████████▌    | 35/39 [00:39<00:04,  1.16s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▋   | 36/39 [00:40<00:03,  1.17s/it]\u001b[A\n",
      " 95%|████████████████████████████████████████▊  | 37/39 [00:41<00:02,  1.17s/it]\u001b[A\n",
      " 97%|█████████████████████████████████████████▉ | 38/39 [00:43<00:01,  1.19s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.5007132291793823, 'eval_cer': 0.4909265818356727, 'eval_wer': 0.8763636363636363, 'eval_runtime': 45.3799, 'eval_samples_per_second': 109.079, 'eval_steps_per_second': 0.859, 'epoch': 1.07}\n",
      " 11%|███▎                           | 15000/139950 [2:26:51<20:02:22,  1.73it/s]\n",
      "100%|███████████████████████████████████████████| 39/39 [00:44<00:00,  1.04s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to /scratch/lingjzhu_root/lingjzhu1/lingjzhu/g2p/mt5_12_layers_baseline/checkpoint-15000\n",
      "Configuration saved in /scratch/lingjzhu_root/lingjzhu1/lingjzhu/g2p/mt5_12_layers_baseline/checkpoint-15000/config.json\n",
      "Model weights saved in /scratch/lingjzhu_root/lingjzhu1/lingjzhu/g2p/mt5_12_layers_baseline/checkpoint-15000/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/lingjzhu_root/lingjzhu1/lingjzhu/g2p/mt5_12_layers_baseline/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/lingjzhu_root/lingjzhu1/lingjzhu/g2p/mt5_12_layers_baseline/checkpoint-15000/special_tokens_map.json\n",
      "Copy vocab file to /scratch/lingjzhu_root/lingjzhu1/lingjzhu/g2p/mt5_12_layers_baseline/checkpoint-15000/spiece.model\n",
      "Deleting older checkpoint [/scratch/lingjzhu_root/lingjzhu1/lingjzhu/g2p/mt5_12_layers_baseline/checkpoint-5000] due to args.save_total_limit\n",
      "{'loss': 1.2324, 'learning_rate': 0.00029145602844994243, 'epoch': 1.14}        \n",
      "{'loss': 1.1904, 'learning_rate': 0.0002902917144067724, 'epoch': 1.21}         \n",
      "{'loss': 1.1553, 'learning_rate': 0.00028905568764860047, 'epoch': 1.29}        \n",
      "{'loss': 1.1214, 'learning_rate': 0.00028774857999345685, 'epoch': 1.36}        \n",
      "{'loss': 1.0929, 'learning_rate': 0.0002863710595936922, 'epoch': 1.43}         \n",
      " 14%|████▍                          | 20000/139950 [3:14:59<19:06:30,  1.74it/s]The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: pron, language, word, variant.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4950\n",
      "  Batch size = 128\n",
      "\n",
      "  0%|                                                    | 0/39 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|██▎                                         | 2/39 [00:01<00:21,  1.70it/s]\u001b[A\n",
      "  8%|███▍                                        | 3/39 [00:02<00:29,  1.21it/s]\u001b[A\n",
      " 10%|████▌                                       | 4/39 [00:03<00:33,  1.04it/s]\u001b[A\n",
      " 13%|█████▋                                      | 5/39 [00:04<00:36,  1.07s/it]\u001b[A\n",
      " 15%|██████▊                                     | 6/39 [00:06<00:36,  1.11s/it]\u001b[A\n",
      " 18%|███████▉                                    | 7/39 [00:07<00:35,  1.12s/it]\u001b[A\n",
      " 21%|█████████                                   | 8/39 [00:08<00:36,  1.16s/it]\u001b[A\n",
      " 23%|██████████▏                                 | 9/39 [00:09<00:34,  1.16s/it]\u001b[A\n",
      " 26%|███████████                                | 10/39 [00:10<00:33,  1.14s/it]\u001b[A\n",
      " 28%|████████████▏                              | 11/39 [00:11<00:32,  1.15s/it]\u001b[A\n",
      " 31%|█████████████▏                             | 12/39 [00:12<00:29,  1.10s/it]\u001b[A\n",
      " 33%|██████████████▎                            | 13/39 [00:13<00:29,  1.12s/it]\u001b[A\n",
      " 36%|███████████████▍                           | 14/39 [00:15<00:28,  1.14s/it]\u001b[A\n",
      " 38%|████████████████▌                          | 15/39 [00:16<00:27,  1.14s/it]\u001b[A\n",
      " 41%|█████████████████▋                         | 16/39 [00:17<00:26,  1.15s/it]\u001b[A\n",
      " 44%|██████████████████▋                        | 17/39 [00:18<00:25,  1.17s/it]\u001b[A\n",
      " 46%|███████████████████▊                       | 18/39 [00:19<00:25,  1.20s/it]\u001b[A\n",
      " 49%|████████████████████▉                      | 19/39 [00:21<00:23,  1.17s/it]\u001b[A\n",
      " 51%|██████████████████████                     | 20/39 [00:22<00:21,  1.15s/it]\u001b[A\n",
      " 54%|███████████████████████▏                   | 21/39 [00:23<00:20,  1.16s/it]\u001b[A\n",
      " 56%|████████████████████████▎                  | 22/39 [00:24<00:19,  1.18s/it]\u001b[A\n",
      " 59%|█████████████████████████▎                 | 23/39 [00:25<00:19,  1.21s/it]\u001b[A\n",
      " 62%|██████████████████████████▍                | 24/39 [00:26<00:17,  1.19s/it]\u001b[A\n",
      " 64%|███████████████████████████▌               | 25/39 [00:28<00:16,  1.17s/it]\u001b[A\n",
      " 67%|████████████████████████████▋              | 26/39 [00:29<00:15,  1.17s/it]\u001b[A\n",
      " 69%|█████████████████████████████▊             | 27/39 [00:30<00:13,  1.15s/it]\u001b[A\n",
      " 72%|██████████████████████████████▊            | 28/39 [00:31<00:12,  1.13s/it]\u001b[A\n",
      " 74%|███████████████████████████████▉           | 29/39 [00:32<00:10,  1.10s/it]\u001b[A\n",
      " 77%|█████████████████████████████████          | 30/39 [00:33<00:10,  1.14s/it]\u001b[A\n",
      " 79%|██████████████████████████████████▏        | 31/39 [00:34<00:09,  1.16s/it]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 32/39 [00:36<00:08,  1.16s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▍      | 33/39 [00:37<00:06,  1.17s/it]\u001b[A\n",
      " 87%|█████████████████████████████████████▍     | 34/39 [00:38<00:05,  1.13s/it]\u001b[A\n",
      " 90%|██████████████████████████████████████▌    | 35/39 [00:39<00:04,  1.15s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▋   | 36/39 [00:40<00:03,  1.16s/it]\u001b[A\n",
      " 95%|████████████████████████████████████████▊  | 37/39 [00:41<00:02,  1.16s/it]\u001b[A\n",
      " 97%|█████████████████████████████████████████▉ | 38/39 [00:43<00:01,  1.18s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2862120866775513, 'eval_cer': 0.4343652980016616, 'eval_wer': 0.82, 'eval_runtime': 45.2469, 'eval_samples_per_second': 109.4, 'eval_steps_per_second': 0.862, 'epoch': 1.43}\n",
      " 14%|████▍                          | 20000/139950 [3:15:44<19:06:30,  1.74it/s]\n",
      "100%|███████████████████████████████████████████| 39/39 [00:44<00:00,  1.04s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to /scratch/lingjzhu_root/lingjzhu1/lingjzhu/g2p/mt5_12_layers_baseline/checkpoint-20000\n",
      "Configuration saved in /scratch/lingjzhu_root/lingjzhu1/lingjzhu/g2p/mt5_12_layers_baseline/checkpoint-20000/config.json\n",
      "Model weights saved in /scratch/lingjzhu_root/lingjzhu1/lingjzhu/g2p/mt5_12_layers_baseline/checkpoint-20000/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/lingjzhu_root/lingjzhu1/lingjzhu/g2p/mt5_12_layers_baseline/checkpoint-20000/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/lingjzhu_root/lingjzhu1/lingjzhu/g2p/mt5_12_layers_baseline/checkpoint-20000/special_tokens_map.json\n",
      "Copy vocab file to /scratch/lingjzhu_root/lingjzhu1/lingjzhu/g2p/mt5_12_layers_baseline/checkpoint-20000/spiece.model\n",
      "Deleting older checkpoint [/scratch/lingjzhu_root/lingjzhu1/lingjzhu/g2p/mt5_12_layers_baseline/checkpoint-10000] due to args.save_total_limit\n",
      "{'loss': 1.06, 'learning_rate': 0.0002849238305944389, 'epoch': 1.5}            \n",
      "{'loss': 1.0365, 'learning_rate': 0.00028340763277367477, 'epoch': 1.57}        \n",
      "{'loss': 1.0107, 'learning_rate': 0.0002818232411640713, 'epoch': 1.64}         \n",
      "{'loss': 0.9896, 'learning_rate': 0.00028017146565682144, 'epoch': 1.71}        \n",
      "{'loss': 0.962, 'learning_rate': 0.00027845315058764886, 'epoch': 1.79}         \n",
      " 18%|█████▌                         | 25000/139950 [4:03:54<18:17:14,  1.75it/s]The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: pron, language, word, variant.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4950\n",
      "  Batch size = 128\n",
      "\n",
      "  0%|                                                    | 0/39 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|██▎                                         | 2/39 [00:01<00:21,  1.72it/s]\u001b[A\n",
      "  8%|███▍                                        | 3/39 [00:02<00:28,  1.25it/s]\u001b[A\n",
      " 10%|████▌                                       | 4/39 [00:03<00:33,  1.05it/s]\u001b[A\n",
      " 13%|█████▋                                      | 5/39 [00:04<00:35,  1.05s/it]\u001b[A\n",
      " 15%|██████▊                                     | 6/39 [00:05<00:36,  1.10s/it]\u001b[A\n",
      " 18%|███████▉                                    | 7/39 [00:07<00:35,  1.11s/it]\u001b[A\n",
      " 21%|█████████                                   | 8/39 [00:08<00:35,  1.15s/it]\u001b[A\n",
      " 23%|██████████▏                                 | 9/39 [00:09<00:34,  1.15s/it]\u001b[A\n",
      " 26%|███████████                                | 10/39 [00:10<00:32,  1.13s/it]\u001b[A\n",
      " 28%|████████████▏                              | 11/39 [00:11<00:31,  1.14s/it]\u001b[A\n",
      " 31%|█████████████▏                             | 12/39 [00:12<00:29,  1.09s/it]\u001b[A\n",
      " 33%|██████████████▎                            | 13/39 [00:13<00:28,  1.10s/it]\u001b[A\n",
      " 36%|███████████████▍                           | 14/39 [00:14<00:28,  1.13s/it]\u001b[A\n",
      " 38%|████████████████▌                          | 15/39 [00:16<00:27,  1.13s/it]\u001b[A\n",
      " 41%|█████████████████▋                         | 16/39 [00:17<00:26,  1.14s/it]\u001b[A\n",
      " 44%|██████████████████▋                        | 17/39 [00:18<00:25,  1.16s/it]\u001b[A\n",
      " 46%|███████████████████▊                       | 18/39 [00:19<00:25,  1.20s/it]\u001b[A\n",
      " 49%|████████████████████▉                      | 19/39 [00:20<00:23,  1.16s/it]\u001b[A\n",
      " 51%|██████████████████████                     | 20/39 [00:21<00:21,  1.14s/it]\u001b[A\n",
      " 54%|███████████████████████▏                   | 21/39 [00:23<00:20,  1.15s/it]\u001b[A\n",
      " 56%|████████████████████████▎                  | 22/39 [00:24<00:20,  1.18s/it]\u001b[A\n",
      " 59%|█████████████████████████▎                 | 23/39 [00:25<00:19,  1.21s/it]\u001b[A\n",
      " 62%|██████████████████████████▍                | 24/39 [00:26<00:17,  1.16s/it]\u001b[A\n",
      " 64%|███████████████████████████▌               | 25/39 [00:27<00:16,  1.15s/it]\u001b[A\n",
      " 67%|████████████████████████████▋              | 26/39 [00:28<00:14,  1.15s/it]\u001b[A\n",
      " 69%|█████████████████████████████▊             | 27/39 [00:30<00:13,  1.13s/it]\u001b[A\n",
      " 72%|██████████████████████████████▊            | 28/39 [00:31<00:11,  1.09s/it]\u001b[A\n",
      " 74%|███████████████████████████████▉           | 29/39 [00:32<00:10,  1.10s/it]\u001b[A\n",
      " 77%|█████████████████████████████████          | 30/39 [00:33<00:10,  1.13s/it]\u001b[A\n",
      " 79%|██████████████████████████████████▏        | 31/39 [00:34<00:09,  1.15s/it]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 32/39 [00:35<00:08,  1.15s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▍      | 33/39 [00:36<00:06,  1.16s/it]\u001b[A\n",
      " 87%|█████████████████████████████████████▍     | 34/39 [00:37<00:05,  1.13s/it]\u001b[A\n",
      " 90%|██████████████████████████████████████▌    | 35/39 [00:39<00:04,  1.14s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▋   | 36/39 [00:40<00:03,  1.16s/it]\u001b[A\n",
      " 95%|████████████████████████████████████████▊  | 37/39 [00:41<00:02,  1.16s/it]\u001b[A\n",
      " 97%|█████████████████████████████████████████▉ | 38/39 [00:42<00:01,  1.17s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.1428043842315674, 'eval_cer': 0.3822860641042459, 'eval_wer': 0.7822222222222223, 'eval_runtime': 44.8058, 'eval_samples_per_second': 110.477, 'eval_steps_per_second': 0.87, 'epoch': 1.79}\n",
      " 18%|█████▌                         | 25000/139950 [4:04:39<18:17:14,  1.75it/s]\n",
      "100%|███████████████████████████████████████████| 39/39 [00:43<00:00,  1.02s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to /scratch/lingjzhu_root/lingjzhu1/lingjzhu/g2p/mt5_12_layers_baseline/checkpoint-25000\n",
      "Configuration saved in /scratch/lingjzhu_root/lingjzhu1/lingjzhu/g2p/mt5_12_layers_baseline/checkpoint-25000/config.json\n",
      "Model weights saved in /scratch/lingjzhu_root/lingjzhu1/lingjzhu/g2p/mt5_12_layers_baseline/checkpoint-25000/pytorch_model.bin\n",
      "tokenizer config file saved in /scratch/lingjzhu_root/lingjzhu1/lingjzhu/g2p/mt5_12_layers_baseline/checkpoint-25000/tokenizer_config.json\n",
      "Special tokens file saved in /scratch/lingjzhu_root/lingjzhu1/lingjzhu/g2p/mt5_12_layers_baseline/checkpoint-25000/special_tokens_map.json\n",
      "Copy vocab file to /scratch/lingjzhu_root/lingjzhu1/lingjzhu/g2p/mt5_12_layers_baseline/checkpoint-25000/spiece.model\n",
      "Deleting older checkpoint [/scratch/lingjzhu_root/lingjzhu1/lingjzhu/g2p/mt5_12_layers_baseline/checkpoint-15000] due to args.save_total_limit\n",
      "{'loss': 0.9412, 'learning_rate': 0.00027666917430520975, 'epoch': 1.86}        \n",
      "{'loss': 0.9218, 'learning_rate': 0.00027482044872210895, 'epoch': 1.93}        \n",
      " 19%|█████▉                         | 27015/139950 [4:24:06<18:03:43,  1.74it/s]"
     ]
    }
   ],
   "source": [
    "!python src/train.py --output_dir /scratch/lingjzhu_root/lingjzhu1/lingjzhu/g2p/mt5_12_layers_baseline --num_encoder_layers 8 --num_decoder_layers 4  --model byt5 --model_name google/mt5-small --train --train_batch_size 64 --gradient_accumulation 8 --eval_batch_size 128"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
